{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "2_o5am_4Qt0b",
    "outputId": "768c7e55-f643-4d38-dbd7-f065ed0c0a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyTigerGraph in /Users/ybenami/EasyAsPie.ai/pyTigerGraph (0.0.6.7)\n",
      "Requirement already satisfied: validators in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pyTigerGraph) (0.15.0)\n",
      "Requirement already satisfied: requests in /Users/ybenami/Library/Python/3.8/lib/python/site-packages (from pyTigerGraph) (2.24.0)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.0.5-cp38-cp38-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from validators->pyTigerGraph) (4.4.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/ybenami/Library/Python/3.8/lib/python/site-packages (from validators->pyTigerGraph) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/ybenami/Library/Python/3.8/lib/python/site-packages (from requests->pyTigerGraph) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ybenami/Library/Python/3.8/lib/python/site-packages (from requests->pyTigerGraph) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->pyTigerGraph) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ybenami/Library/Python/3.8/lib/python/site-packages (from requests->pyTigerGraph) (2020.6.20)\n",
      "Collecting numpy>=1.13.3\n",
      "  Downloading numpy-1.19.0-cp38-cp38-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.3 MB 8.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->pyTigerGraph) (2.8.1)\n",
      "Installing collected packages: numpy, pytz, pandas\n",
      "Successfully installed numpy-1.19.0 pandas-1.0.5 pytz-2020.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyTigerGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zl0dpmNQQPV"
   },
   "source": [
    "## Connect to a Synthea TigerGraph server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qbhwL35mQWaR",
    "outputId": "acc2c480-e424-452f-b552-bebc7c91e6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "server = 'https://yaniv.i.tgcloud.io'\n",
    "password = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZWZeNsmkQqgL",
    "outputId": "aefc4a3b-dfd6-41eb-c33c-1f79324a9f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gsql client Jar\n",
      "Downloading SSL Certificate\n"
     ]
    }
   ],
   "source": [
    "import pyTigerGraph as tg \n",
    "\n",
    "conn = tg.TigerGraphConnection(\n",
    "    host=server, \n",
    "    graphname=\"synthea\", \n",
    "    username=\"tigergraph\",\n",
    "    password=password,\n",
    ")\n",
    "\n",
    "shell = tg.Gsql(conn, client_version=\"2.6.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "M0IQC9CDRNGE",
    "outputId": "692b9ee2-8c52-4776-ef00-5f8b0a7ee433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1432\r\n",
      "-rw-r--r--  1 ybenami  ybenami  671144 Jul 21 14:06 gsql_client.jar\r\n",
      "-rw-r--r--  1 ybenami  ybenami    8518 Jul 21 14:06 my-cert.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l $shell.jarLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0HSEhFPQ88M"
   },
   "outputs": [],
   "source": [
    "#!rm -rf $shell.jarLocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PQIeUyiGyOvl",
    "outputId": "507b374c-079f-43d0-c128-7298dfe556fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qjv4m187po3f21abtoki1u94d5bemeh0', 1597950428, '2020-08-20 19:07:08')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secret = shell.createSecret()\n",
    "conn.getToken(secret=secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "E7ppOsw8wVuB",
    "outputId": "18424e00-fac1-4c25-fe6d-76d00fe8fad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Trying version: v2_6_0\n",
      "Connecting to yaniv.i.tgcloud.io:14240\n",
      "If there is any relative path, it is relative to tigergraph/dev/gdk/gsql\n",
      "The query get_all_patients is dropped.\n",
      "The query get_all_patients has been added!\n",
      "Start installing queries, about 1 minute ...\n",
      "get_all_patients query: curl -X GET 'https://127.0.0.1:9000/query/synthea/get_all_patients'. Add -H \"Authorization: Bearer TOKEN\" if authentication is enabled.\n",
      "\n",
      "\r",
      "[                                                                 ] 0% (0/1)   \r",
      "[                                                                 ] 0% (0/1)   \r",
      "[===============================                                  ] 47% (0/1)  \r",
      "[==============================================================   ] 95% (0/1)  \r",
      "[=================================================================] 100% (1/1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "drop query get_all_patients\n",
    "create query get_all_patients() for graph synthea{\n",
    "    TYPEDEF TUPLE <description STRING, date DATETIME> CondTup;\n",
    "\n",
    "    BagAccum <CondTup> @conditions;\n",
    "\n",
    "    patients = {Patient.*};\n",
    "\n",
    "    patients = select pat from patients:pat;\n",
    "\n",
    "    x = select pat from patients:pat-(PATIENT_HAS_CONDITION)-Condition:cond\n",
    "            accum pat.@conditions += CondTup(cond.description, cond.startDate);\n",
    "\n",
    "    print patients;\n",
    "}\n",
    "install query get_all_patients\n",
    "'''\n",
    "\n",
    "print(shell.gsql(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aiT2EGmd3g-E"
   },
   "outputs": [],
   "source": [
    "query = conn.runInstalledQuery('get_all_patients', sizeLimit=10**10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G3-ImRrqp-2y",
    "outputId": "cd677c61-597b-4efb-a7fb-356ebfd17d8b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109321"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query[0]['patients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "axsVyL7aSBuy",
    "outputId": "ccbb0fed-3cdc-49d2-80f1-48ce5632d720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting getFeatures.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile getFeatures.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "def yaniv_to_ed_query(query):\n",
    "\n",
    "    query[0]['people'] = query[0].pop('patients')\n",
    "\n",
    "    for patient in query[0]['people']:\n",
    "        patient['attributes']['people.dateOfBirth'] = patient['attributes'].pop('birth')\n",
    "        patient['attributes']['people.dateOfDeath'] = patient['attributes'].pop('death')\n",
    "        patient['attributes']['people.@gender'] = [(\n",
    "            'F' if patient['attributes'].pop('gender') == 'female'\n",
    "            else 'M')]\n",
    "        patient['attributes']['people.@diagData'] = patient['attributes'].pop('@conditions')\n",
    "        for condition in patient['attributes']['people.@diagData']:\n",
    "            condition['diagnosisDate'] = condition.pop('date')\n",
    "            condition['diagnosis'] = condition.pop('description')\n",
    "        patient['attributes'].pop('name')\n",
    "        patient['attributes'].pop('patient_id')\n",
    "        \n",
    "    return query\n",
    "\n",
    "\n",
    "def get_conditions(query, startDate='1900-01-01', endDate='2019-12-31'):\n",
    "        \n",
    "    startDate = datetime.strptime(startDate, '%Y-%m-%d')\n",
    "    endDate = datetime.strptime(endDate, '%Y-%m-%d')\n",
    "\n",
    "    conditions = [\n",
    "        condition['diagnosis']\n",
    "        for patient in query[0]['people'] \n",
    "        for condition in patient['attributes']['people.@diagData']\n",
    "        if (\n",
    "            datetime.strptime(condition['diagnosisDate'], '%Y-%m-%d %H:%M:%S') \n",
    "            >= startDate\n",
    "            and datetime.strptime(condition['diagnosisDate'], '%Y-%m-%d %H:%M:%S')\n",
    "            <= endDate\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    conditions = pd.Series(conditions).value_counts()    \n",
    "\n",
    "    return conditions\n",
    "\n",
    "\n",
    "def get_live_patients(query, startDate='1900-01-01', endDate='2019-12-31'):\n",
    "\n",
    "    startDate = datetime.strptime(startDate, '%Y-%m-%d')\n",
    "    endDate = datetime.strptime(endDate, '%Y-%m-%d')\n",
    "\n",
    "    patients = [\n",
    "        patient['v_id'] \n",
    "        for patient in query[0]['people']\n",
    "        if (\n",
    "            datetime.strptime(\n",
    "                patient['attributes']['people.dateOfBirth'], '%Y-%m-%d %H:%M:%S') \n",
    "            <= endDate\n",
    "            and datetime.strptime(\n",
    "                patient['attributes']['people.dateOfDeath'], '%Y-%m-%d %H:%M:%S')\n",
    "            >= startDate\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return patients\n",
    "\n",
    "def make_age_groups(years = 5, top_year = 100):\n",
    "    age_groups_ranges = [(i,min(i+years-1,top_year)) \n",
    "        for i in range(0, top_year, years)] + [(top_year, 140)]\n",
    "\n",
    "    age_group_titles = [\n",
    "        'Age {}-{}'.format(start, end) for start, end in age_groups_ranges]\n",
    "\n",
    "    return age_groups_ranges, age_group_titles\n",
    "\n",
    "\n",
    "def get_feature_vec(query, conditions, startDate, endDate, age_groups):\n",
    "\n",
    "    startDate = datetime.strptime(startDate, '%Y-%m-%d')\n",
    "    endDate = datetime.strptime(endDate, '%Y-%m-%d')\n",
    "\n",
    "    demog_df = pd.DataFrame([patient['attributes'] \n",
    "                             for patient in query[0]['people']])\n",
    "\n",
    "    demog_df.index = [\n",
    "        patient['v_id'] for patient in query[0]['people']\n",
    "    ]\n",
    "\n",
    "    demog_df = demog_df[[\n",
    "        'people.@gender',\n",
    "        'people.dateOfBirth',\n",
    "        'people.dateOfDeath',\n",
    "    ]]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "            np.zeros((len(demog_df.index), len(conditions.index))),\n",
    "            index=demog_df.index, \n",
    "            columns=conditions.index,\n",
    "        )\n",
    "\n",
    "    for patient in query[0]['people']:\n",
    "\n",
    "        patient_conditions = [\n",
    "            condition['diagnosis'] \n",
    "            for condition in patient['attributes']['people.@diagData']\n",
    "            if (\n",
    "                datetime.strptime(condition['diagnosisDate'], '%Y-%m-%d %H:%M:%S') \n",
    "                >= startDate\n",
    "                and datetime.strptime(condition['diagnosisDate'], '%Y-%m-%d %H:%M:%S')\n",
    "                <= endDate\n",
    "                and condition['diagnosis'] in conditions\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        df.loc[patient['v_id'], patient_conditions ] = 1\n",
    "\n",
    "    return concat_features(\n",
    "        conditions_df=df, \n",
    "        demog_df=demog_df, \n",
    "        date=endDate, \n",
    "        age_groups=age_groups,\n",
    "    )\n",
    "\n",
    "def concat_features(conditions_df, demog_df, date, age_groups):\n",
    "\n",
    "    dead_df = deceased(demog_df, date)\n",
    "    gender_df = gender(demog_df)\n",
    "    age_df = age_group_df(\n",
    "        df = demog_df, \n",
    "        date_for_age = date,\n",
    "        age_groups=age_groups,\n",
    "    )\n",
    "    \n",
    "    return pd.concat([gender_df, dead_df, age_df, conditions_df], axis=1)\n",
    "\n",
    "\n",
    "def age_group_df(df, date_for_age, age_groups):\n",
    "\n",
    "    age_group_df = pd.DataFrame(\n",
    "        np.zeros((len(df),len(age_groups[0]))),\n",
    "        index=df.index, \n",
    "        columns=age_groups[1]\n",
    "        )\n",
    "\n",
    "    for i in df.index:\n",
    "        age = math.floor(\n",
    "            (date_for_age\n",
    "            - datetime.strptime(df.loc[i,'people.dateOfBirth'], '%Y-%m-%d %H:%M:%S')\n",
    "            ).days/365.25\n",
    "        )\n",
    "\n",
    "        for j, age_group in enumerate(age_groups[0]):\n",
    "            if age >= age_group[0] and age <= age_group[1]:\n",
    "                age_group_df.loc[i].iloc[j]=1\n",
    "\n",
    "    return age_group_df\n",
    "\n",
    "\n",
    "def deceased(df, date):\n",
    "\n",
    "    dead = df['people.dateOfDeath'].apply(\n",
    "        lambda x: 1.0 if (\n",
    "            date - datetime.strptime(x, '%Y-%m-%d %H:%M:%S')\n",
    "        ).days > 0 else 0.0\n",
    "    )\n",
    "\n",
    "    dead.name = 'Deceased'\n",
    "\n",
    "    return dead\n",
    "\n",
    "\n",
    "def gender(df):\n",
    "\n",
    "    gender = df['people.@gender'].apply(\n",
    "        lambda x: 1.0 if x[0]=='F' else 0.0\n",
    "    )\n",
    "\n",
    "    gender.name = 'Female'\n",
    "\n",
    "    return gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2Kg51Yyicd7W",
    "outputId": "5b7ea33c-fee4-4118-82c1-6bee3393e912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'getFeatures' from '/Users/ybenami/EasyAsPie.ai/MedGraphML/getFeatures.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getFeatures\n",
    "import importlib\n",
    "\n",
    "importlib.reload(getFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N_XG4kR0zDRu",
    "outputId": "33c4f4f7-a1dd-4acc-878e-53e00176a39d"
   },
   "outputs": [],
   "source": [
    "query = getFeatures.yaniv_to_ed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Br0WqPEYR1U_"
   },
   "source": [
    "## Get the set of all conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "jE_qN9OJR8Yh",
    "outputId": "595dbfc1-8952-4f69-902d-3ae75f36c002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting predictConditions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile predictConditions.py\n",
    "import getFeatures\n",
    "import feature_weighted_mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from scipy.stats import ttest_1samp    \n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def predictConditions(query):\n",
    "    \n",
    "    print('Collecting all conditions:\\n')\n",
    "    \n",
    "    conditions = getFeatures.get_conditions(\n",
    "        query=query, startDate='2019-01-01', endDate='2020-12-31')\n",
    "    \n",
    "    print(conditions)\n",
    "    \n",
    "\n",
    "    patients = getFeatures.get_live_patients(\n",
    "        query=query, startDate='2019-12-31', endDate='2019-12-31')\n",
    "    \n",
    "    print('\\nNumber of patients', len(patients))\n",
    "    \n",
    "\n",
    "    age_groups = getFeatures.make_age_groups()\n",
    "    \n",
    "    print('\\nAge groups\\n', age_groups)\n",
    "    \n",
    "\n",
    "    print('\\nCompute features: ')\n",
    "    \n",
    "    x_df = getFeatures.get_feature_vec(\n",
    "        query,\n",
    "        conditions=conditions,\n",
    "        startDate='2019-01-01', \n",
    "        endDate='2019-12-31', \n",
    "        age_groups=age_groups)\n",
    "\n",
    "    print('\\nx_df.shape ', x_df.shape)\n",
    "    \n",
    "    print('\\nCompute labels: ')\n",
    "    \n",
    "    y_df = getFeatures.get_feature_vec(\n",
    "        query,\n",
    "        conditions=conditions,\n",
    "        startDate='2020-01-01', \n",
    "        endDate='2020-12-31', \n",
    "        age_groups=age_groups)\n",
    "    \n",
    "    print('\\ny_df.shape ', y_df.shape)\n",
    "    \n",
    "\n",
    "    train, test = train_test_split(patients, test_size=0.25, random_state=42)\n",
    "    \n",
    "    x_train_df = x_df.loc[train]\n",
    "    y_train_df = y_df.loc[train]\n",
    "    x_test_df = x_df.loc[test]\n",
    "    y_test_df = y_df.loc[test]\n",
    "    \n",
    "    print('\\n\\nTrain set:', len(train), 'Test set: ', len(test))\n",
    "    \n",
    "    print(\n",
    "        '\\n\\nSorted x_train means:\\n\\n',\n",
    "        x_train_df.mean().sort_values(ascending=False), \n",
    "        '\\n\\nSorted y_train means:\\n\\n',\n",
    "        y_train_df.mean().sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    filter_below = 20\n",
    "    print('\\nFiltereing conditions with less than {} cases:'.format(filter_below))\n",
    "    \n",
    "    x_drop_list = ( \n",
    "        set(x_train_df.columns[x_train_df.sum() < filter_below])\n",
    "        | set(x_test_df.columns[x_train_df.sum() < filter_below])\n",
    "    )\n",
    "\n",
    "    x_train_df = x_train_df.drop(x_drop_list, axis=1)\n",
    "    x_test_df = x_test_df.drop(x_drop_list, axis=1)\n",
    "\n",
    "    y_drop_list = ( \n",
    "        set(y_train_df.columns[y_train_df.sum() < filter_below])\n",
    "        | set(y_test_df.columns[y_train_df.sum() < filter_below])\n",
    "    )\n",
    "\n",
    "    y_train_df = y_train_df.drop(y_drop_list, axis=1)\n",
    "    y_test_df = y_test_df.drop(y_drop_list, axis=1)\n",
    "\n",
    "    print(\n",
    "        '\\n\\nSorted x_train means:\\n\\n',\n",
    "        x_train_df.mean().sort_values(ascending=False), \n",
    "        '\\n\\nSorted y_train means:\\n\\n\\n\\n',\n",
    "        y_train_df.mean().sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(\n",
    "        '\\n\\nSorted x_test means:\\n\\n',\n",
    "        x_test_df.mean().sort_values(ascending=False), \n",
    "        '\\n\\nSorted y_test means:\\n\\n\\n\\n',\n",
    "        y_test_df.mean().sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    y_weights = 1 / (y_train_df.var() + 1e-3)\n",
    "    y_weights = y_weights/(y_train_df.var()*y_weights).sum()\n",
    "    \n",
    "    print(\n",
    "        '\\n',\n",
    "        pd.DataFrame(\n",
    "            [y_train_df.var(), y_weights, y_weights*y_train_df.var()],\n",
    "             index=['y_train var', 'y_weights', 'var*weight']\n",
    "        ).transpose()\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    wmse = feature_weighted_mse.make_feature_weighted_mse(y_weights)\n",
    "    \n",
    "    print(\n",
    "        '\\nBasic benchmark - y means\\n', \n",
    "        'Train loss',\n",
    "        wmse(\n",
    "            y_true=y_train_df.values, \n",
    "            y_pred=y_train_df.values.mean(axis=0)\n",
    "        ).numpy().mean(),\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "    n_splits = 4\n",
    "    n_repeats = 2\n",
    "    alpha=0.0001\n",
    "    learning_rate=0.001\n",
    "    patience=10\n",
    "    \n",
    "    print('\\nTrain linear model using Lasso alpha {} {}-fold CV repeated {} times.\\n'.format(\n",
    "        alpha, n_splits, n_repeats,\n",
    "    ))\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "    \n",
    "    models=[]\n",
    "    history=[]\n",
    "    performance=[]\n",
    "    \n",
    "    i=0\n",
    "    for train_index, validate_index in rkf.split(x_train_df):\n",
    "        \n",
    "        i += 1\n",
    "        print('\\n\\nFold {} out of {}\\n\\n'.format(i, n_splits*n_repeats))\n",
    "        \n",
    "        x_train, x_validate = x_train_df.iloc[train_index], x_train_df.iloc[validate_index]\n",
    "        y_train, y_validate = y_train_df.iloc[train_index], y_train_df.iloc[validate_index]\n",
    "    \n",
    "        inputs = keras.layers.Input(shape=x_train_df.shape[1])\n",
    "        outputs = keras.layers.Dense(\n",
    "            units=y_train_df.shape[1], \n",
    "            kernel_regularizer=keras.regularizers.l1(l=alpha),\n",
    "        )(inputs)\n",
    "        \n",
    "        models.append(keras.Model(inputs=inputs, outputs=outputs))\n",
    "\n",
    "        models[-1].compile(loss=wmse, optimizer=keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "        history.append(models[-1].fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=128,\n",
    "            epochs=1000,\n",
    "            validation_data=(x_validate, y_validate),\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),\n",
    "            ]\n",
    "        ))\n",
    "    \n",
    "        print('\\nEvaluate on test set:\\n')\n",
    "        performance.append(models[-1].evaluate(x=x_test_df, y=y_test_df))\n",
    "        print(performance[-1],'\\n')\n",
    "\n",
    "    print('Test loss mean', np.mean(performance), 'std' , np.std(performance, ddof=1))\n",
    "    \n",
    "    \n",
    "    constant_df = pd.DataFrame(\n",
    "        np.array([model.layers[1].get_weights()[1] for model in models]).transpose(), \n",
    "        index=y_train_df.columns, \n",
    "        columns=['Fold {}'.format(i) for i in range(1, 1+n_splits*n_repeats)],\n",
    "    )\n",
    "    \n",
    "    constant_name = pd.Series(constant_df.index, index=constant_df.index, name='name')\n",
    "        \n",
    "    constant_mean = constant_df.mean(axis=1)\n",
    "    constant_mean.name = 'mean'\n",
    "    \n",
    "    constant_std = constant_df.std(axis=1, ddof=1)\n",
    "    constant_std.name = 'std'\n",
    "    \n",
    "    constant_df = pd.concat(\n",
    "        [constant_name, constant_mean, constant_std], axis=1)\n",
    "    \n",
    "    constant_df['p-value'] = (1 - t.cdf(\n",
    "        x=abs(constant_mean/constant_std), df=n_splits*n_repeats-1)) * 2\n",
    "    \n",
    "    constant_df['FDR adj p-value'] = multipletests(constant_p_value, method='fdr_bh')[1]\n",
    "    \n",
    "    constant_df.to_csv('constant.csv', index=False)\n",
    "    print('\\n\\nConstants:\\n\\n ', constant_df.head(30))\n",
    "\n",
    "\n",
    "    coef_mat = np.array([model.layers[1].get_weights()[0] for model in models]).transpose((1, 2, 0))\n",
    "    \n",
    "    coef_df = pd.DataFrame(\n",
    "        [[json.dumps(coef_mat[i,j].tolist()) \n",
    "          for j in range(coef_mat.shape[1])] \n",
    "         for i in range(coef_mat.shape[0])], \n",
    "        columns=y_train_df.columns, \n",
    "        index=x_train_df.columns\n",
    "    ).transpose()\n",
    "    \n",
    "    coef_mean = coef_df.applymap(lambda x: np.mean(json.loads(x)))\n",
    "    coef_mean.to_csv('coef_mean.csv')\n",
    "    \n",
    "    coef_std = coef_df.applymap(lambda x: np.std(json.loads(x), ddof=1))\n",
    "    coef_std.to_csv('coef_std.csv')\n",
    "    \n",
    "    coef_p_value = pd.DataFrame(\n",
    "        (1 - t.cdf(x=abs(coef_mean/coef_std), df=n_splits*n_repeats-1)) * 2,\n",
    "        index=coef_df.index,\n",
    "        columns=coef_df.columns\n",
    "    )\n",
    "    coef_p_value.to_csv('coef_p_value.csv')\n",
    "    \n",
    "    coef_p_value_adj = multipletests(\n",
    "        coef_p_value.values.reshape((-1,)), \n",
    "        method='fdr_bh')[1].reshape(coef_p_value.shape)\n",
    "    \n",
    "    coef_p_value_adj = pd.DataFrame(\n",
    "        coef_p_value_adj, index=y_train_df.columns, columns=x_train_df.columns)\n",
    "    coef_p_value_adj.to_csv('coef_p_value_adj.csv')\n",
    "\n",
    "    nonzero = coef_p_value_adj.applymap(lambda x: x <= 0.05).values.nonzero()\n",
    "\n",
    "    tuples = [(\n",
    "        coef_df.index[i], \n",
    "        coef_df.columns[j], \n",
    "        coef_mean.iloc[i,j],\n",
    "        coef_std.iloc[i,j],\n",
    "        coef_p_value.iloc[i,j],\n",
    "        coef_p_value_adj.iloc[i,j],\n",
    "    ) for i,j in zip(nonzero[0],nonzero[1])]\n",
    "    \n",
    "    coef_df = pd.DataFrame(tuples, columns=['y','x','mean', 'std', 'p-value', 'FDR adj p-value'])\n",
    "    coef_df.to_csv('coef.csv', index=False)\n",
    "    print(coef_df.head(30))\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(80, 50))\n",
    "    ax = sns.heatmap(\n",
    "        coef_mean, \n",
    "        xticklabels=x_train_df.columns, \n",
    "        yticklabels=y_train_df.columns,\n",
    "        center=0.0,\n",
    "        cmap='seismic',\n",
    "    )\n",
    "    \n",
    "    plt.savefig('linear_coefs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "Ru6eJ3AzVNPz",
    "outputId": "d909015e-6078-42e5-b1dd-a083db713c91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'predictConditions' from '/Users/ybenami/EasyAsPie.ai/MedGraphML/predictConditions.py'>"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import predictConditions\n",
    "import importlib\n",
    "\n",
    "importlib.reload(predictConditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "colab_type": "code",
    "id": "ZTYckAhjI8Ty",
    "outputId": "8bf7acbd-a004-4cd8-ffc3-c85bafccdbe8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#predictConditions.predictConditions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3dYRxNjYgMT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting feature_weighted_mse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile feature_weighted_mse.py\n",
    "import tensorflow as tf\n",
    "\n",
    "def make_feature_weighted_mse(feature_weights):\n",
    "    \n",
    "    feature_weights = tf.reshape(tf.cast(feature_weights, 'float32'), (-1,1))\n",
    "    \n",
    "    def feature_weighted_mse(y_true, y_pred):\n",
    "        \n",
    "        y_true = tf.cast(y_true, 'float32')\n",
    "        y_pred = tf.cast(y_pred, 'float32')\n",
    "\n",
    "        return tf.linalg.matmul(tf.square(y_true - y_pred), feature_weights)\n",
    "        #tf.reduce_sum(\n",
    "    \n",
    "    return feature_weighted_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'feature_weighted_mse' from '/Users/ybenami/EasyAsPie.ai/MedGraphML/feature_weighted_mse.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feature_weighted_mse\n",
    "\n",
    "importlib.reload(feature_weighted_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from scipy.stats import ttest_1samp    \n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z3dYRxNjYgMT"
   },
   "outputs": [],
   "source": [
    "    print('Collecting all conditions:\\n')\n",
    "    \n",
    "    conditions = getFeatures.get_conditions(\n",
    "        query=query, startDate='2019-01-01', endDate='2020-12-31')\n",
    "    \n",
    "    print(conditions)\n",
    "    \n",
    "\n",
    "    patients = getFeatures.get_live_patients(\n",
    "        query=query, startDate='2019-12-31', endDate='2019-12-31')\n",
    "    \n",
    "    print('\\nNumber of patients', len(patients))\n",
    "    \n",
    "\n",
    "    age_groups = getFeatures.make_age_groups()\n",
    "    \n",
    "    print('\\nAge groups\\n', age_groups)\n",
    "    \n",
    "\n",
    "    print('\\nCompute features: ')\n",
    "    \n",
    "    x_df = getFeatures.get_feature_vec(\n",
    "        query,\n",
    "        conditions=conditions,\n",
    "        startDate='2019-01-01', \n",
    "        endDate='2019-12-31', \n",
    "        age_groups=age_groups)\n",
    "\n",
    "    print('\\nx_df.shape ', x_df.shape)\n",
    "    \n",
    "    print('\\nCompute labels: ')\n",
    "    \n",
    "    y_df = getFeatures.get_feature_vec(\n",
    "        query,\n",
    "        conditions=conditions,\n",
    "        startDate='2020-01-01', \n",
    "        endDate='2020-12-31', \n",
    "        age_groups=age_groups)\n",
    "    \n",
    "    print('\\ny_df.shape ', y_df.shape)\n",
    "    \n",
    "\n",
    "    train, test = train_test_split(patients, test_size=0.25, random_state=42)\n",
    "    \n",
    "    x_train_df = x_df.loc[train]\n",
    "    y_train_df = y_df.loc[train]\n",
    "    x_test_df = x_df.loc[test]\n",
    "    y_test_df = y_df.loc[test]\n",
    "    \n",
    "    print('\\n\\nTrain set:', len(train), 'Test set: ', len(test))\n",
    "    \n",
    "    print(\n",
    "        '\\n\\nSorted x_train means:\\n\\n',\n",
    "        x_train_df.mean().sort_values(ascending=False), \n",
    "        '\\n\\nSorted y_train means:\\n\\n',\n",
    "        y_train_df.mean().sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    filter_below = 20\n",
    "    print('\\nFiltereing conditions with less than {} cases:'.format(filter_below))\n",
    "    \n",
    "    x_drop_list = ( \n",
    "        set(x_train_df.columns[x_train_df.sum() < filter_below])\n",
    "        | set(x_test_df.columns[x_train_df.sum() < filter_below])\n",
    "    )\n",
    "\n",
    "    x_train_df = x_train_df.drop(x_drop_list, axis=1)\n",
    "    x_test_df = x_test_df.drop(x_drop_list, axis=1)\n",
    "\n",
    "    y_drop_list = ( \n",
    "        set(y_train_df.columns[y_train_df.sum() < filter_below])\n",
    "        | set(y_test_df.columns[y_train_df.sum() < filter_below])\n",
    "    )\n",
    "\n",
    "    y_train_df = y_train_df.drop(y_drop_list, axis=1)\n",
    "    y_test_df = y_test_df.drop(y_drop_list, axis=1)\n",
    "\n",
    "    print(\n",
    "        '\\n\\nSorted x_train means:\\n\\n',\n",
    "        x_train_df.mean().sort_values(ascending=False), \n",
    "        '\\n\\nSorted y_train means:\\n\\n\\n\\n',\n",
    "        y_train_df.mean().sort_values(ascending=False)\n",
    "    )\n",
    "    \n",
    "    print(\n",
    "        '\\n\\nSorted x_test means:\\n\\n',\n",
    "        x_test_df.mean().sort_values(ascending=False), \n",
    "        '\\n\\nSorted y_test means:\\n\\n\\n\\n',\n",
    "        y_test_df.mean().sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    y_weights = 1 / (y_train_df.var() + 1e-3)\n",
    "    y_weights = y_weights/(y_train_df.var()*y_weights).sum()\n",
    "    \n",
    "    print(\n",
    "        '\\n',\n",
    "        pd.DataFrame(\n",
    "            [y_train_df.var(), y_weights, y_weights*y_train_df.var()],\n",
    "             index=['y_train var', 'y_weights', 'var*weight']\n",
    "        ).transpose()\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    wmse = feature_weighted_mse.make_feature_weighted_mse(y_weights)\n",
    "    \n",
    "    print(\n",
    "        '\\nBasic benchmark - y means\\n', \n",
    "        'Train loss',\n",
    "        wmse(\n",
    "            y_true=y_train_df.values, \n",
    "            y_pred=y_train_df.values.mean(axis=0)\n",
    "        ).numpy().mean(),\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "    n_splits = 4\n",
    "    n_repeats = 2\n",
    "    alpha=0.0001\n",
    "    learning_rate=0.001\n",
    "    patience=10\n",
    "    \n",
    "    print('\\nTrain linear model using Lasso alpha {} {}-fold CV repeated {} times.\\n'.format(\n",
    "        alpha, n_splits, n_repeats,\n",
    "    ))\n",
    "    \n",
    "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "    \n",
    "    models=[]\n",
    "    history=[]\n",
    "    performance=[]\n",
    "    \n",
    "    i=0\n",
    "    for train_index, validate_index in rkf.split(x_train_df):\n",
    "        \n",
    "        i += 1\n",
    "        print('\\n\\nFold {} out of {}\\n\\n'.format(i, n_splits*n_repeats))\n",
    "        \n",
    "        x_train, x_validate = x_train_df.iloc[train_index], x_train_df.iloc[validate_index]\n",
    "        y_train, y_validate = y_train_df.iloc[train_index], y_train_df.iloc[validate_index]\n",
    "    \n",
    "        inputs = keras.layers.Input(shape=x_train_df.shape[1])\n",
    "        outputs = keras.layers.Dense(\n",
    "            units=y_train_df.shape[1], \n",
    "            kernel_regularizer=keras.regularizers.l1(l=alpha),\n",
    "        )(inputs)\n",
    "        \n",
    "        models.append(keras.Model(inputs=inputs, outputs=outputs))\n",
    "\n",
    "        models[-1].compile(loss=wmse, optimizer=keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "        history.append(models[-1].fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=128,\n",
    "            epochs=1000,\n",
    "            validation_data=(x_validate, y_validate),\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(patience=patience, restore_best_weights=True),\n",
    "            ]\n",
    "        ))\n",
    "    \n",
    "        print('\\nEvaluate on test set:\\n')\n",
    "        performance.append(models[-1].evaluate(x=x_test_df, y=y_test_df))\n",
    "        print(performance[-1],'\\n')\n",
    "\n",
    "    print('Test loss mean', np.mean(performance), 'std' , np.std(performance, ddof=1))\n",
    "    \n",
    "    \n",
    "    constant_df = pd.DataFrame(\n",
    "        np.array([model.layers[1].get_weights()[1] for model in models]).transpose(), \n",
    "        index=y_train_df.columns, \n",
    "        columns=['Fold {}'.format(i) for i in range(1, 1+n_splits*n_repeats)],\n",
    "    )\n",
    "    \n",
    "    constant_name = pd.Series(constant_df.index, index=constant_df.index, name='name')\n",
    "        \n",
    "    constant_mean = constant_df.mean(axis=1)\n",
    "    constant_mean.name = 'mean'\n",
    "    \n",
    "    constant_std = constant_df.std(axis=1, ddof=1)\n",
    "    constant_std.name = 'std'\n",
    "    \n",
    "    constant_df = pd.concat(\n",
    "        [constant_name, constant_mean, constant_std], axis=1)\n",
    "    \n",
    "    constant_df['p-value'] = (1 - t.cdf(\n",
    "        x=abs(constant_mean/constant_std), df=n_splits*n_repeats-1)) * 2\n",
    "    \n",
    "    constant_df['FDR adj p-value'] = multipletests(constant_p_value, method='fdr_bh')[1]\n",
    "    \n",
    "    constant_df.to_csv('constant.csv', index=False)\n",
    "    print('\\n\\nConstants:\\n\\n ', constant_df.head(30))\n",
    "\n",
    "\n",
    "    coef_mat = np.array([model.layers[1].get_weights()[0] for model in models]).transpose((1, 2, 0))\n",
    "    \n",
    "    coef_df = pd.DataFrame(\n",
    "        [[json.dumps(coef_mat[i,j].tolist()) \n",
    "          for j in range(coef_mat.shape[1])] \n",
    "         for i in range(coef_mat.shape[0])], \n",
    "        columns=y_train_df.columns, \n",
    "        index=x_train_df.columns\n",
    "    ).transpose()\n",
    "    \n",
    "    coef_mean = coef_df.applymap(lambda x: np.mean(json.loads(x)))\n",
    "    coef_mean.to_csv('coef_mean.csv')\n",
    "    \n",
    "    coef_std = coef_df.applymap(lambda x: np.std(json.loads(x), ddof=1))\n",
    "    coef_std.to_csv('coef_std.csv')\n",
    "    \n",
    "    coef_p_value = pd.DataFrame(\n",
    "        (1 - t.cdf(x=abs(coef_mean/coef_std), df=n_splits*n_repeats-1)) * 2,\n",
    "        index=coef_df.index,\n",
    "        columns=coef_df.columns\n",
    "    )\n",
    "    coef_p_value.to_csv('coef_p_value.csv')\n",
    "    \n",
    "    coef_p_value_adj = multipletests(\n",
    "        coef_p_value.values.reshape((-1,)), \n",
    "        method='fdr_bh')[1].reshape(coef_p_value.shape)\n",
    "    \n",
    "    coef_p_value_adj = pd.DataFrame(\n",
    "        coef_p_value_adj, index=y_train_df.columns, columns=x_train_df.columns)\n",
    "    coef_p_value_adj.to_csv('coef_p_value_adj.csv')\n",
    "\n",
    "    nonzero = coef_p_value_adj.applymap(lambda x: x <= 0.05).values.nonzero()\n",
    "\n",
    "    tuples = [(\n",
    "        coef_df.index[i], \n",
    "        coef_df.columns[j], \n",
    "        coef_mean.iloc[i,j],\n",
    "        coef_std.iloc[i,j],\n",
    "        coef_p_value.iloc[i,j],\n",
    "        coef_p_value_adj.iloc[i,j],\n",
    "    ) for i,j in zip(nonzero[0],nonzero[1])]\n",
    "    \n",
    "    coef_df = pd.DataFrame(tuples, columns=['y','x','mean', 'std', 'p-value', 'FDR adj p-value'])\n",
    "    coef_df.to_csv('coef.csv', index=False)\n",
    "    print(coef_df.head(30))\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(80, 50))\n",
    "    ax = sns.heatmap(\n",
    "        coef_mean, \n",
    "        xticklabels=x_train_df.columns, \n",
    "        yticklabels=y_train_df.columns,\n",
    "        center=0.0,\n",
    "        cmap='seismic',\n",
    "    )\n",
    "    \n",
    "    plt.savefig('linear_coefs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Filter all patients.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
